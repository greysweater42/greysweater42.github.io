<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data engineering on greaysweater42&#39;s cookbook</title>
    <link>https://greysweater42.github.io/categories/data-engineering/</link>
    <description>Recent content in Data engineering on greaysweater42&#39;s cookbook</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 16 May 2024 11:56:13 +0200</lastBuildDate>
    
	<atom:link href="https://greysweater42.github.io/categories/data-engineering/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>airflow &#43; BigQuery</title>
      <link>https://greysweater42.github.io/airflow_bigquery/</link>
      <pubDate>Thu, 16 May 2024 11:56:13 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/airflow_bigquery/</guid>
      <description>Why would you use airflow with BigQuery? Because they both work great for creating ELT processes, storing huge amounts of data, and providing a convenient interface to these data in SQL.
A couple of years ago I wrote a short blog post about airflow 1. I also wrote about BigQuery and how it works with terraform. You might want to have a look there as well.
Prerequsites Before you start working with airflow + GCP, you need to install and configure airflow (I recommend using virtualenv for that):</description>
    </item>
    
    <item>
      <title>terraform &#43; bigquery</title>
      <link>https://greysweater42.github.io/terraform_bigquery/</link>
      <pubDate>Mon, 13 May 2024 07:50:45 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/terraform_bigquery/</guid>
      <description>Why would you use terraform in data engineering? terraform is a standard in industry for setting up cloud infrastructure. You might wonder, just as I do, whether you should store your DDLs in terraform: it certainly is possible, but there are some drawbacks, e.g. how to handle database schema migrations? And how does it cooperate with other tools, like Apache Airflow? Surely there are interesing alternatives, like dbt or dataform.</description>
    </item>
    
    <item>
      <title>RabbimtMQ</title>
      <link>https://greysweater42.github.io/rabbimtmq/</link>
      <pubDate>Sat, 24 Jun 2023 13:16:44 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/rabbimtmq/</guid>
      <description>1. What is RabbitMQ and why do we need it? RabbitMQ is a message broker, which means that it enables communication between services. Probably the most popular way of communication is TCP, but it has a couple of drawbacks, which rabbitmq solves, for example:
 lower coupling between services, so when consumer is down, the messages are not lost lots of messages at once don&amp;rsquo;t overwhelm the consumer: it can process them one by one at its own pace  and also provides additional features:</description>
    </item>
    
    <item>
      <title>async/threads</title>
      <link>https://greysweater42.github.io/async_thread/</link>
      <pubDate>Wed, 29 Sep 2021 00:14:48 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/async_thread/</guid>
      <description>1. What does &amp;ldquo;asynchronous&amp;rdquo; and &amp;ldquo;threading&amp;rdquo; mean and why is it important? Let&amp;rsquo;s begin with the definition of processes and threads provided Gerald of Stackoverflow:
 A process is a collection of code, memory, data and other resources. A thread is a sequence of code that is executed within the scope of the process. You can (usually) have multiple threads executing concurrently within the same process.
 Asynchrounous usually refers to using many processes, while threading - many threads within one process.</description>
    </item>
    
    <item>
      <title>useful processing</title>
      <link>https://greysweater42.github.io/useful_processing/</link>
      <pubDate>Fri, 17 May 2019 15:44:38 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/useful_processing/</guid>
      <description>1. What is useful processing?  many machine learning algorithms require the same kinds of data preprocessing in order for them to work properly. In other words, theses kinds of processing are useful.   2. Examples one-hot encoding R
# data.table dt_iris &amp;lt;- data.table::as.data.table(iris) mltools::one_hot(dt_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species_setosa ## 1: 5.1 3.5 1.4 0.2 1 ## 2: 4.9 3.0 1.4 0.2 1 ## 3: 4.7 3.</description>
    </item>
    
    <item>
      <title>pandas</title>
      <link>https://greysweater42.github.io/pandas/</link>
      <pubDate>Fri, 25 Jan 2019 13:46:12 +0100</pubDate>
      
      <guid>https://greysweater42.github.io/pandas/</guid>
      <description>1. What is pandas and why would you use it?  pandas is probably your first choice for working with tabular data in Python. True, there is also datatable, but itâ€™s nowhere near as popular as pandas.
 Effectively it is the only reasonable Python package for this purpose, which makes Python a little modest comparing to R (base, data.table, dplyr - every one of them has a better interface than pandas) for table processing.</description>
    </item>
    
    <item>
      <title>hadoop</title>
      <link>https://greysweater42.github.io/hadoop/</link>
      <pubDate>Tue, 04 Dec 2018 21:32:35 +0100</pubDate>
      
      <guid>https://greysweater42.github.io/hadoop/</guid>
      <description>1. What is hadoop and why would you use it?   hadoop is the first ever popular big data tool;
  it lets you quickly compute huge amounts of data thanks to dividing computation into many machines i.e. a cluster of machines; (quickly comparing to a standard, one-machine approach);
  you can store and easily access huge amounts of data thanks to hadoop&amp;rsquo;s distributed file system (hdfs);</description>
    </item>
    
    <item>
      <title>spark</title>
      <link>https://greysweater42.github.io/spark/</link>
      <pubDate>Fri, 23 Nov 2018 12:58:49 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/spark/</guid>
      <description>1. What is spark and why would use use it?   Spark is a smooth framework for working with big data, i.e. hdfs;
  it can be accessed from Python, R, scala (spark is actually written in scala) and java;
  it is probably the most popular big data tool nowadays for data scientists.
  2. A few &amp;ldquo;Hello World&amp;rdquo; examples a) pyspark Prerequisites Installation of pyspark In this tutorial we will work on a development python version of spark.</description>
    </item>
    
    <item>
      <title>logging</title>
      <link>https://greysweater42.github.io/logging/</link>
      <pubDate>Sat, 20 Oct 2018 00:15:21 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/logging/</guid>
      <description>1. What is logging and why would you use it?   Logging, in general, sends information about the execution of a program to the outside of the program, e.g. to stdout or to a file. Why would that be useful?
  You may get the information of how and when the program was executed, e.g. who was using it&amp;rsquo;s functionalities and if all the pieces of your program finished correctly.</description>
    </item>
    
    <item>
      <title>airflow</title>
      <link>https://greysweater42.github.io/airflow/</link>
      <pubDate>Tue, 14 Aug 2018 11:51:12 +0200</pubDate>
      
      <guid>https://greysweater42.github.io/airflow/</guid>
      <description>1. What is airflow and why would you use it?  airflow manages dataflow as a graph (direct acyclic graph or DAG), which consists of separate tasks, and schedule them   Wait, you may say, I can do that with cron!
 Yes, you can, but with airflow:
  airflow implements a well-known pipeline design pattern with DAGs, which are data engineer&amp;rsquo;s workhorses
  you can easily divide your app into smaller tasks and monitor their reliability and execution duration</description>
    </item>
    
  </channel>
</rss>