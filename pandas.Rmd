---
title: "pandas"
date: 2019-01-25T13:46:12+01:00
draft: false
categories: ["Python", "Data engineering"]
---


## 1. What is `pandas` and why would you use it?

* `pandas` is probably your first choice for working with tabular data in Python. True, there is also [datatable](https://datatable.readthedocs.io/en/latest/), but it's nowhere near as popular as pandas.

* Effectively it is the only reasonable Python package for this purpose, which makes Python a little modest comparing to R (base, data.table, dplyr - every one of them has a better interface than pandas) for table processing.

Personally I find `pandas` a tool that:

- makes any task moderately-simple. This may be an advantage for difficult tasks, but for everyday use is slightly frustrating.

- has weird defaults, e.g.:

    - `write_csv` has `index=True` by default. Who would want that? Especially that `read_csv` does not read the column `index` as index.

    - `groupby` has `as_index=True` by default, which creates MultiIndexes, which seem to be completely useless (or is it just me who doesn't use them?)

Even though I use pandas almost every day, there are certain solutions that I constantly forget about. Here's this short blog post with useful snippets.


## 2. Snippets

### datetime

Let's work on a simple dataframe:

```{python, engine.path = '/usr/bin/python3'}
import pandas as pd

df = pd.DataFrame(dict(event_id=[1, 2], time=["2020-01-01 12:00", "2020-01-02 12:00"]))
print(df)
print(df.info())
```

The first thing that you should do is converting the string in to datetime:

```{python, engine.path = '/usr/bin/python3'}
df["time"] = pd.to_datetime(df["time"])
print(df)
print(df.info())
```

which provides interesting methods available with `.dt`

```{python, engine.path = '/usr/bin/python3'}
print(df["time"].dt.date)
print(df["time"].dt.year)
```

`to_datetime` creates objects of type `pandas.Timestamp`, which have interesting methods described in [pandas docs](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html), like `replace`:

```{python, engine.path = '/usr/bin/python3'}
print(type(df["time"][0]))
new_times = df["time"].map(lambda x: x.replace(month=2, year=2021))
print(new_times)
```

Filtering is rather straightforward:

```{python, engine.path = '/usr/bin/python3'}
print(df["time"] > "2020-01-02 00:01")
# but if you want to be meticulous, you can do it with
print(df["time"] > pd.Timestamp(2020, 1, 2, 0, 1))
```

To sum up: you should convert your datetime strings to `pandas.Timestamp` with `pd.to_datetime`. Since then you have two possible types of operations:

- using `.dt` property, which may be useful for filtering,

- using `.map` and `lambda` for manipulating `pd.Timestamp` objects one by one.


### grouping and aggregation

or a task that you would expect should be easy enough, yet in pandas it is not.

Let's define a simple dataframe...

```{python, engine.path = '/usr/bin/python3'}
import numpy as np
import pandas as pd

df = pd.DataFrame({
    "event_id": [1, 1, 2, 2, 2],
    "result": [12, 10, 15, 11, np.nan],
    "comment": ["good", "bad", "good", "good", "bad"]
})
print(df)
```

...and group it by the column "event_id":

```{python, engine.path = '/usr/bin/python3'}
grouped = df.groupby("event_id")
print(type(grouped))
```

The resulting object is of type `DataFrameGroupBy` and has an interesting property: we can iterate over it:

```{python, engine.path = '/usr/bin/python3'}
for g in grouped:
    print(g)
```

Interestingly, each item is a tuple of `(id, df_sub)`, where `id` is the name of the column, by which we group, and `df_sub` is a subset of `df`, where `df[df.id == i]` and `i` is one of `set(df["id"])`. For many grouping columns though, the things get even more interesting: 
```{python, engine.path = '/usr/bin/python3'}
grouped = df.groupby(["event_id", "comment"])
for (event_id, comment), df_sub in grouped:
    print(f"df for event_id: {event_id} and comment: {comment}:")
    print(df_sub, "\n")
```

It seems that grouping only divides the dataset into subsets based on the argument `by` of the function.

But there is one minor issue with grouping: I often get frustrated with multiindexes, so I use `as_index=False`
```{python, engine.path = '/usr/bin/python3'}
grouped = df.groupby(["event_id", "comment"], as_index=False)
for (event_id, comment), df_sub in grouped:
    print(f"df for event_id: {event_id} and comment: {comment}:")
    print(df_sub, "\n")
```

that's better.

When the data is grouped, we can easily obtain sizes/counts of each group. Counting can be done just like that:
```{python, engine.path = '/usr/bin/python3'}
print(grouped.size(), "\n")
print(grouped.count(), "\n")  # excludes missing values - result for event 2 has count = 2
print(df.count(), "\n")  # count() works also for the whole df
```

### aggregating

`.agg` may be used for a dataframe or a groupby object. Here are some examples for a dataframe:

```{python, engine.path = '/usr/bin/python3'}
print(df.agg(min), "\n")
print(df.agg([min, max, 'mean', np.std]), "\n")
print(df.agg({'event_id': ['max', 'mean'], 'result': 'std'}), "\n")  # some funny formatting, different to the one returned by groupby.agg
```

and for grouped data

```{python, engine.path = '/usr/bin/python3'}
grouped = df.groupby(["event_id", "comment"])
print(grouped.agg(min), "\n")
print(grouped.agg([min, max, 'mean', np.std]), "\n")
# useful for grouping many columns
print(grouped.agg({'result': [min, max, 'mean', np.std]}), "\n")
```

As you can see, we obtained multiindexes in rows, which are IMHO redundant (they could be columns). Unfortunately `as_index=False` stops working when aggregating, but we can use .reset_index() instead.

```{python, engine.path = '/usr/bin/python3'}
grouped = df.groupby(["event_id", "comment"])
print(grouped.agg({'result': [min, 'mean', np.std]}), "\n")
```

Besides, for many metrics we get a multicolumn index, which may be problematic, so it is a good idea to flatten it:

```{python, engine.path = '/usr/bin/python3'}
res = grouped.agg([min, max, 'mean', np.std]).reset_index()
res.columns = ["_".join(x) for x in res.columns.values]
print(res)
```

Lat but not least, you .agg can takes as arguments functions in various forms:

```{python, engine.path = '/usr/bin/python3'}
def mad(x):
    """ mean absolut deviation of vector x """
    return np.sum(np.abs(x - np.median(x))) / len(x)

functions = [min, 'max', 'std', np.median, lambda x: np.std(x) / np.mean(x), mad]
print(grouped.agg({'result': functions}))
```

To sum up, grouping and aggregating works a little like list comprehensions (do sth for each item) or map-reduce: `groupby` divides the dataset into items, and `agg` runs a function on each of the items.

## 3. Other interesting commands/arguments:

- [select_dtypes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html): `df.select_dtypes(include="object")` or `df.select_dtypes(exlude="object")` chooses onlt those columns, which fulfils criterium
- [rename](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html): `df.rename(columns={"current_name": "new_name", ...})` - instead of saying `rename(..., axis=1)`, you can say `df.rename(columns={...})`, which is a little bit more readable
` 
- [categorical data](https://pandas.pydata.org/docs/user_guide/categorical.html): `df['some_class'].astype('category')` - keeping categorial data as `category` type is more memory-efficient, as values are stored as integers, and their names ar stored in something similar to a `dict`

- [replace - not str.replace](https://pandas.pydata.org/docs/reference/api/pandas.Series.replace.html): `series.replace('other', 'unknown')` - exact synonym to `series.loc[series == "other"] = "unknown"`, but shorter, and you can also use regex like this: `pd.Series(["ab", "ac", "bc"]).replace("a.*", "zz", regex=True)`, which replaces `ab` and `ac` with `zz`

- [str.get()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.get.html): `series.str.get(-1)` - gets the last element of an itarable object, which is *in* the series: if it is a string - takes the last letter, if a list/tuple - the last element of that list/tuple

- [str.split](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html): `series.str.split('_', n=2, expand=True)` - this command splits the strings by underscore, but only to the second (`n=2`) occurence of underscore, and `expand`s the result into several (in this case: 3) columns. An interesting variation of `expand` is `explode`: `series.explode()`, which converts a series of lists into a series by concatenating lists, so the result is a series, in opposite to `str.split(..., expand=True)`, where the result is a dataframe

- [str.replace](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html): `series.str.replace("a.*", "zz", regex=True)` - replace everything that starts with `a` with `zz`; `regex=True` is mandatory in order to use regex

- [str.match](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.match.html): `series.str.match("a.*")` - marks all the values that start with `a` as True, the rest is False. In contrast to `str.replace`, `str.match` uses rregex by default

- [nlargest](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nlargest.html): `df.nlargest(5, columns="class1")` - returns 5 rows with the largest values of class1

- [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html): `pd.read_csv` has milions of parameters, but one of them is particularily interesting: `pd.read_csv(..., dtype={"col1": int})`, which automatically reads the data in a proper format. This can be useful if you don't vae an ORM to store information on data types, yet you want the columns to have proper dtypes, even if the whole column is empty (which may happen if you download data from spark)

- axis: it is possible to write `axis=0` or `axis=1`, but `axis="rows"` and `axis="columns"` are much more readable


## 4. References

I highly recommend this book: [Pandas in action](https://www.manning.com/books/pandas-in-action), to all the readers.